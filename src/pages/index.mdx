---
layout: ../layouts/Layout.astro
title: "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics"
description: Webpage for RoboSpatial
favicon: favicon.ico
# thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

{/* Image imports */}
import fig1 from "../assets/figure1.png";
import fig2 from "../assets/figure2.png";
import fig3 from "../assets/figure3.png";
import robot_exp from "../assets/figure_robot.png";
import demo_video from "../assets/demo_robospatial.mp4"



<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Chan Hee Song",
      url: "https://chanh.ee",
      institution: "The Ohio State University",
      notes: ["*", "+"],
    },
    {
      name: "Valts Blukis",
      institution: "NVIDIA",
      url: "https://research.nvidia.com/person/valts-blukis",
      notes: [],
    },
    {
      name: "Jonathan Tremblay",
      institution: "NVIDIA",
      url: "https://research.nvidia.com/person/jonathan-tremblay",
      notes: [],
    },
    {
      name: "Stephen Tyree",
      institution: "NVIDIA",
      url: "https://research.nvidia.com/person/stephen-tyree",
    },
    {
      name: "Yu Su",
      url: "https://ysu1989.github.io/",
      institution: "The Ohio State University",
    },
    {
      name: "Stan Birchfield",
      url: "https://sbirchfield.github.io/",
      institution: "NVIDIA",
    },
  ]}
  conference="CVPR 2025 Oral (0.74%)"
  notes={[
    {
      symbol: "*",
      text: "Work partly done during Nvidia internship",
    },
    {
      symbol: "+",
      text: "Contact: song.1855@osu.edu",
    },
  ]}
  links={[
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2411.16537",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "https://github.com/NVlabs/RoboSpatial",
      icon: "mdi:github",
      disabled: false,
    },
    {
      name: "Benchmark",
      url: "https://huggingface.co/datasets/chanhee-luke/RoboSpatial-Home",
      icon: "mdi:database",
      disabled: false
    },
  ]}
  />

<div className="center-text">
  **TL;DR** A large-scale 2D/3D dataset of real indoor/tabletop environments for spatial reasoning in robotics.
</div>

<Video source={demo_video} />


<HighlightedSection>

## Overview

{/* **TL;DR** 1) A new training dataset and benchmark, **RoboSpatial** and **RoboSpatial-Home**, comprising images and 3D scans paired with spatial questions and answers, designed for robotics. 2) **RoboSpatial** trained model outperforms prior SOTA VLMs on natural language-specified robot manipulation tasks and indoor spatial scene question answering. */}

Spatial understanding is essential for robots to perceive, reason about, and interact with their environments. However, current visual language models often rely on general-purpose image datasets that lack robust spatial scene understanding and reference frame comprehension (ego-, world-, or object-centric). To address this gap, we introduce **RoboSpatial**, a large-scale dataset of **real indoor and tabletop environments** captured via egocentric images and 3D scans. RoboSpatial provides **1M images, 5k 3D scans, and 3M annotated spatial relationships**, enabling both 2D and 3D spatial reasoning. Models trained on RoboSpatial outperform baselines on tasks including spatial affordance prediction, spatial relationship prediction, and robot manipulation.

</HighlightedSection>




{/* ## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left" caption="Take a look at this YouTube video.">
    <YouTubeVideo videoId="wjZofJX0v4M" />
  </Figure>
  <Figure slot="right" caption="Now look at this Gaussian Splat, rendered with a React component.">
    <Splat client:idle />
  </Figure>
</TwoColumns> */}

## Approach

<Figure
    caption="We automatically generate spatial relationship annotations from existing datasets with 3D point clouds, egocentric images, and 3D bounding box annotations. We create question/answer pairs covering three classes of spatial relationships, three spatial reference frames, and both binary (yes/no) and numeric (e.g. 2D image points) answers. From 1M images and 5k 3D scans, we generate over 3M spatial question/answer pairs."
  >
    <Image source={fig2} altText="An overview of RoboSpatial Dataset." />
</Figure>


## Application Example


<Figure
    caption="An illustration of a model trained on RoboSpatial being used to solve a manipulation task using spatial reasoning."
  >
    <Image source={fig1} altText="An illustration of a model trained on RoboSpatial being used to solve a manipulation task using spatial reasoning." />
</Figure>


## Evaluation Results

### Spatial QA

<Figure
    caption="In-domain (RoboSpatial-Val, top) and out-of-domain (RoboSpatial-Home, BLINK, middle and bottom) results for RoboSpatial-trained models. Two models shown: SL (SpaceLLaVA) and RP (RoboPoint); the -FT suffix indicates fine-tuning on RoboSpatial. Correct answers in green. All images except bottom-right in the out-of-domain rows are from RoboSpatial-Home."
  >
    <Image source={fig3} altText="Qualitative Results of RoboSpatial-trained models." />
</Figure>

<Table caption="Performance of RoboSpatial-trained models on four spatial reasoning benchmarks.">
| Model                       | RoboSpatial-Val | RoboSpatial-Home | BLINK-Spatial | SpatialBench-Position |
|:----------------------------|:---------------:|:-----------------:|:-------------:|:------------:|
| **Open-source**             |                 |                  |               |              |
| *--2D--*                    |                 |                  |               |              |
| LLaVA-NeXT (8B)             | 30.3            | 46.3             | 71.8          |    55.9      |
| + RoboSpatial               | 60.5            | 59.6             | **79.0**      |    **70.6**      |
| RoboPoint (13B)             | 38.9            | 53.4             | 63.6          |    44.1      |
| + RoboSpatial               | 70.6            | **63.4**         | 70.6          |    64.7      |
| *--3D--*                    |                 |                  |               |              |
| Embodied Generalist (7B)    | 42.8            | 29.8             | N/A           |    N/A       |
| + RoboSpatial               | **71.9**        | 43.8             | N/A           |    N/A       |
| **Baselines**               |                 |                  |               |              |
| Molmo (7B)                  | 50.1            | 25.6             | 67.1          |    55.9      |
| GPT-4o                      | 50.8            | 47.0             | 76.2          |    **70.6**      |

</Table>

### Robot Manipulation

<Figure
    caption="Robot experiments: the red dot shows the model output (if not present, the model failed to provide a valid point in the image);  green dots are used to show when a model outputs multiple points. The robot motion generator, cuRobo, is used to grasp the item referenced by the generated point. The spatial- prefix indicates model trained with RoboSpatial."
  >
    <Image source={robot_exp} altText="Robot experiment results." />
</Figure>

<Table caption="Task success rate for robot manipulation.">
| Model                        | Success Rate (%) |
|:----------------------------|:---------------:|
| **Open-source**              |                 |
| LLaVA-NeXT (8B)              | 23.7           |
| + RoboSpatial                | **52.6**    |
| **Baselines** |            |
| Molmo (7B)                  | 43.8           |
| GPT-4o                   | 46.9           |
</Table>

{/* | RoboPoint                 | 44.7           |
| + RoboSpatial                | 46.2        | */}


## BibTeX Citation

```bibtex
@inproceedings{song2025robospatial,
  author    = {Song, Chan Hee and Blukis, Valts and Tremblay, Jonathan and Tyree, Stephen and Su, Yu and Birchfield, Stan},
  title     = {{RoboSpatial}: Teaching Spatial Understanding to {2D} and {3D} Vision-Language Models for Robotics},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2025},
  note      = {To appear},
}
```
