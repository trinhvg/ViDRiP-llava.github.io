---
layout: ../layouts/Layout.astro
title: "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning"
description: Webpage for VideoPath-LLaVA
favicon: favicon.svg
# thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

{/* Image imports */}
import fig1 from "../assets/figure1.png";
import fig2 from "../assets/figure2.png";
import fig3 from "../assets/figure3.png";
import robot_exp from "../assets/figure_robot.png";
import demo_video from "../assets/demo_robospatial.mp4"
import fig_network from "../assets/Network.png";
import fig_table from "../assets/result.png";
import fig_example from "../assets/Example_results.png";
import example from "../assets/Example.png";
import fig_data from "../assets/Data.png";



<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Trinh Vuong",
      url: "https://www.linkedin.com/in/trinh-vuong-218812193/",
      institution: "Korea University",
    },
    {
      name: "Jin Tae Kwak",
      url: "https://www.kwaklab.net/PI.html",
      institution: "Korea University",
    },
  ]}
  conference="Arxiv 2025"
  notes={[
    {
      text: "Contact: jkwak@korea.ac.kr",
    },
  ]}
  links={[
    {
      name: "Code",
      url: "https://github.com/trinhvg/VideoPath-LLaVA",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2505.04192",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Benchmark (Coming Soon)",
      url: "",
        icon: "mdi:database",
    }
  ]}
  />

## Abstract
<div className="center-text">
We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.
Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube.
Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning.
</div>




<HighlightedSection>

## Overview

We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.
Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube.
Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning.

</HighlightedSection>



## Model

VideoPath-LLaVa model
<Figure>
  <Image slot="figure" source={fig_network} altText="Diagram of the transformer deep learning architecture."  />
  <span slot="caption">Overview of the VideoPath-LLaVA model architecture and its freezing and tuning strategy at each stage. The model comprises three main components: a vision encoder (ViT), a projector, and a large language decoder (LLM).</span>
</Figure>


## Data Preparation

<Figure>
  <Image slot="figure" source={fig_data} altText="Data Preparation." />
  <span slot="caption"> Data Preparation </span>
</Figure>


## Examples

<Figure>
  <Image slot="figure" source={fig_example} altText="Diagram of the transformer deep learning architecture." />
  <span slot="caption">Comparison of VideoPath-LLaVA and GPT-4o in the visual reasoning task for diagnosing high-grade serous carcinoma. While both models correctly identify serous carcinoma, GPT-4o fails to recognize key features such as nuclear atypia and desmoplastic stroma, which are critical for assessing tumor invasiveness, leading to a less
precise grading of malignancy.</span>
</Figure>





## Results

<Figure>
  <Image slot="figure" source={fig_table} altText="Diagram of the transformer deep learning architecture." />
  <span slot="caption">Performance benchmarking of text generation models on VideoPath-Instruct.</span>
</Figure>



## BibTeX Citation

```bibtex
@misc{vuong2025videopathllavapathologydiagnosticreasoning,
      title={VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning},
      author={Trinh T. L. Vuong and Jin Tae Kwak},
      year={2025},
      eprint={2505.04192},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.04192},
}
```
